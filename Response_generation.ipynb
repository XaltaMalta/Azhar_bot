{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Response_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzH0edVRF4Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, GRU\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWaR6hJYKd4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "n_a = 64\n",
        "n_s = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcmJYFL4C7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words=30000\n",
        "max_length=30\n",
        "Tx = max_length\n",
        "Ty = max_length\n",
        "\n",
        "path = 'Data/'\n",
        "\n",
        "dirlist = os.listdir(path)\n",
        "human_sentences=[]\n",
        "machine_sentences=[]\n",
        "for File in dirlist:\n",
        "    with open(path+\"/\"+File, 'r') as raw_lines:\n",
        "        lineList = []\n",
        "        while True:        \n",
        "            line = raw_lines.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            lineList.append(line)\n",
        "\n",
        "    for i in range(0, len(lineList)):\n",
        "        if(i%2)==0:\n",
        "            human_sentences.append(lineList[i])\n",
        "        else:\n",
        "            machine_sentences.append(lineList[i])    \n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(human_sentences)\n",
        "human_word_index = tokenizer.word_index\n",
        "human_reverse_word_index = {a:b for (b,a) in human_word_index.items()}\n",
        "tokenizerE=tokenizer\n",
        "\n",
        "tokenizer2 = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "    \n",
        "tokenizer2.fit_on_texts(machine_sentences)\n",
        "machine_word_index = tokenizer2.word_index\n",
        "machine_reverse_word_index = {a:b for (b,a) in machine_word_index.items()}\n",
        "\n",
        "human_sequences = tokenizer.texts_to_sequences(human_sentences)\n",
        "human_padded = tf.keras.preprocessing.sequence.pad_sequences(human_sequences, maxlen=max_length, padding='post', truncating=\"post\")\n",
        "\n",
        "machine_sequences = tokenizer2.texts_to_sequences(machine_sentences)\n",
        "machine_padded = tf.keras.preprocessing.sequence.pad_sequences(machine_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "\n",
        "X = human_padded\n",
        "Y = machine_padded\n",
        "human_vocab = human_word_index\n",
        "reverse_human_vocab = human_reverse_word_index\n",
        "machine_vocab = machine_word_index\n",
        "reverse_machine_vocab = machine_reverse_word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJuFy19B_Yg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taking care of odd-even shit\n",
        "if X.shape[0]>Y.shape[0]:\n",
        "  X = np.delete(X, len(X)-1, axis=0)\n",
        "elif X.shape[0]<Y.shape[0]:\n",
        "  Y = Y.delete(Y, len(Y)-1, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu8Ruv-eKIwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X,Y)).shuffle(BATCH_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K560oH2laxXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d3614ce5-8c2c-4062-b9a1-5cb51795405b"
      },
      "source": [
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2269   94 1250 ...    0    0    0]\n",
            " [3778 3779 1375 ...    0    0    0]\n",
            " [ 527  114  154 ...    0    0    0]\n",
            " ...\n",
            " [   2   42   15 ...    0    0    0]\n",
            " [  32    0    0 ...    0    0    0]\n",
            " [  52   46   26 ...    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGk3236slTJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, batch_size=100, dim=max_length, shuffle=False):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.len_per_epoch = int(len(X)/self.batch_size)\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.len_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        p,q = self.__data_generation()\n",
        "\n",
        "        return p, q\n",
        "\n",
        "    def __data_generation(self):\n",
        "        \n",
        "        X = np.empty((self.batch_size, self.dim))\n",
        "        y = np.empty((self.batch_size, self.dim))\n",
        "        s0 = np.zeros((self.batch_size, n_s))\n",
        "        c0 = np.zeros((self.batch_size, n_s))\n",
        "        X, y = next(iter(dataset))\n",
        "        y = tf.transpose(y)\n",
        "\n",
        "        return [X,s0,c0], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivk54BuP4OSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "e520122e-6a3f-4b0d-9cf6-e5fa5908d46c"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-30 14:49:43--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2020-06-30 14:49:44--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2020-06-30 14:49:45--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  2.05MB/s    in 11m 44s \n",
            "\n",
            "2020-06-30 15:01:29 (2.06 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BX16zAwFr_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4c6516c5-5c1f-4f90-92ae-14733b19a961"
      },
      "source": [
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJk96ubIwKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2991f266-d5f9-4051-c4df-a7ac377326c1"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1193514 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8pn-RZJ3Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim=200\n",
        "embedding_matrix = np.zeros((len(human_vocab)+1, embedding_dim))\n",
        "for word, i in human_vocab.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QShnjewCKSdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddingLayer = tf.keras.layers.Embedding((len(human_vocab)+1),embedding_dim, weights=[embedding_matrix], trainable=False, input_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrXB-BAUA2Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor = Dense(1, activation = \"relu\")\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gv7cw1LaAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \n",
        "    s_prev = repeator(s_prev)\n",
        "    \n",
        "    concat = concatenator([a, s_prev])\n",
        "    \n",
        "    e = densor(concat)\n",
        "    \n",
        "    alphas = tf.nn.softmax(e, axis=1)\n",
        "    \n",
        "    context = dotor([alphas, a])\n",
        "    \n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu0L4QAvFrCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6iE5J-DvDYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def include_yhat(context,out):\n",
        "  reduced = [tf.math.argmax(out,axis=1)]\n",
        "  reduced = tf.convert_to_tensor(reduced)\n",
        "  reduced = tf.transpose(reduced)\n",
        "  reduced = tf.cast(reduced, tf.dtypes.float32)\n",
        "  concatable = RepeatVector(1)(reduced)\n",
        "  concat = Concatenate(axis=-1)([context,concatable])\n",
        "  g = Dense(1, activation='relu')(concat)\n",
        "  alphas = tf.nn.softmax(g, axis=1)\n",
        "  feedable = Dot(axes=1)([alphas,context])\n",
        "  return feedable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSBOmec1G29j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \n",
        "    X = Input(shape=(Tx,))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    outputs = []\n",
        "\n",
        "    x = embeddingLayer(X)\n",
        "    print(\"Shape of x after embedding:\")\n",
        "    print(x.shape)\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(x)\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Create context\n",
        "        context = one_step_attention(a, s)\n",
        "        if t!=0:          \n",
        "          context = include_yhat(context,out)\n",
        "        # Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
        "        \n",
        "        # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
        "        p = Dense(machine_vocab_size)(s)\n",
        "\n",
        "        out = tf.nn.softmax(p,axis=1)       \n",
        "        \n",
        "        # Append \"out\" to the \"outputs\" list and covert it into tf tensor later\n",
        "        outputs.append(out)\n",
        "    \n",
        "    outputs = tf.convert_to_tensor(outputs)\n",
        "    reduced = tf.math.argmax(out,axis=1)\n",
        "    reduced = tf.convert_to_tensor(reduced)\n",
        "    reduced = tf.transpose(reduced)\n",
        "    reduced = tf.cast(reduced, tf.dtypes.float32)\n",
        "    concatable = RepeatVector(1)(reduced)\n",
        "    concat = Concatenate(axis=-1)([context,concatable])\n",
        "    g = Dense(1, activation='relu')(concat)\n",
        "    alphas = tf.nn.softmax(g, axis=1)\n",
        "    feedable = Dot(axes=1)([alphas,context])    \n",
        "\n",
        "    print(\"SEEEEEEEEEEEEEEEEE:\",feedable.shape)\n",
        "\n",
        "    # Create model instance taking three inputs and returning the tensor of outputs\n",
        "    model = Model([X, s0, c0], outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIudXpVIoYDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "344291ac-8dfa-4de2-bb7a-2054b21ded84"
      },
      "source": [
        "model = model(Tx,Ty,n_a,n_s,len(human_vocab),len(machine_vocab))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5a16d4af8fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wP8Vzsfcs2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0025)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXrWZwsDfI-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "f592efab-4a36-4d46-ac13-efcd69a9f360"
      },
      "source": [
        "generator = DataGenerator()\n",
        "model.fit_generator(generator=generator, epochs=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "458/458 [==============================] - 126s 274ms/step - loss: 1.2365\n",
            "Epoch 2/7\n",
            "458/458 [==============================] - 126s 275ms/step - loss: 1.0258\n",
            "Epoch 3/7\n",
            "458/458 [==============================] - 126s 276ms/step - loss: 0.9826\n",
            "Epoch 4/7\n",
            "458/458 [==============================] - 126s 276ms/step - loss: 0.5695\n",
            "Epoch 5/7\n",
            "458/458 [==============================] - 126s 276ms/step - loss: 0.1521\n",
            "Epoch 6/7\n",
            "458/458 [==============================] - 127s 276ms/step - loss: 0.0464\n",
            "Epoch 7/7\n",
            "458/458 [==============================] - 126s 275ms/step - loss: 0.0274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05f0a7fa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWCdqVsaXcX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Examples = ['What are you doing?','Hey','Who are you?','Hope you are having a nice day']\n",
        "sequenceE = tokenizer.texts_to_sequences(Examples)\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(sequences=sequenceE, maxlen=max_length, padding='post', truncating = \"post\")\n",
        "print(X)\n",
        "print(X.shape)\n",
        "s0 = np.zeros((len(Examples),n_s))\n",
        "c0 = np.zeros((len(Examples),n_s))\n",
        "prediction = model.predict([X,s0,c0])\n",
        "perm = [1,0,2]\n",
        "prediction = tf.transpose(prediction, perm=perm)\n",
        "responses = []\n",
        "prediction = np.array(prediction)\n",
        "\n",
        "for i in range(0,prediction.shape[0]):\n",
        "  temp = []\n",
        "  for j in range(0, prediction.shape[1]):\n",
        "    p = np.argmax(prediction[i][j])\n",
        "    temp.append(p)\n",
        "  responses.append(temp)\n",
        "\n",
        "final_responses = []\n",
        "print(responses)\n",
        "for k in range(0, len(responses)):\n",
        "  tempo = []\n",
        "  for l in range(0,len(responses[k])):\n",
        "    if responses[k][l]!=0:\n",
        "      kkk = reverse_machine_vocab[responses[k][l]]\n",
        "      tempo.append(kkk)\n",
        "  final_responses.append(tempo)\n",
        "print(\"\\n\")\n",
        "for m in range(0,len(final_responses)):\n",
        "  print(Examples[m],\"\\n\")\n",
        "  print(\"-->\",' '.join(final_responses[m]),\"\\n\\n\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCq-yVgo206A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCMdNeF16PiI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1a1c16b5-f97d-43f2-c43b-9a7de0fbd5b7"
      },
      "source": [
        "!tar -zxvf Downloads.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.user.Zone.Identifier'\n",
            "ldamallet_model\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.user.Zone.Identifier'\n",
            "corpus.pickle\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.user.Zone.Identifier'\n",
            "id2word.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3LZLtij1tT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35ee158b-d499-46db-b486-95258dc7ff50"
      },
      "source": [
        "with open('corpus.pickle','rb') as f:\n",
        "  corpus = pickle.load(f)\n",
        "with open('id2word.pickle','rb') as j:\n",
        "  id2word = pickle.load(j)\n",
        "  \n",
        "print(id2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(68470 unique tokens: ['day', 'get', 'third', 'also', 'blah']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43rg1HyhOWny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "8c0a5409-8ab4-45ea-940a-d3331977795d"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=25, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cac8a2be2d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                            per_word_topics=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    975\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mPosterior\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FfEhOOPIDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}